# -*- coding: utf-8 -*-
"""G27 Python Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L2YZkukgWgvQNFG5wlQ_ZzlyNdUlKzkN

# Effectiveness of Sentiment Analysis on twitter data for predicting outcomes of US Presidential Election (2020)

## Group-27
## Suryansh Bhatnagar (21116092), Chetan Sharma (21116031), Rahul Negi (21116078), & Yawalkar Ajinkya Ganpati (21116108)

Importing some useful libraries required for executing the code
"""

import numpy as np
import pandas as pd
import re
import nltk
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

"""Importing Dataset for trump and biden using pandas"""

df_trump = pd.read_csv("hashtag_donaldtrump.csv",lineterminator='\n')
df_biden = pd.read_csv("hashtag_joebiden.csv",lineterminator='\n')

df_trump.head()

len(df_trump)

df_biden.head()

len(df_biden)

df_trump['Tweet_subject'] = 'Trump' #created a column in both datasets 'Tweet_subject' which is used later
df_biden['Tweet_subject'] = 'Biden'

"""We have removed some unecessary columns from dataset that might not impact our final result. We have removed columns like TweetId, continent, UserLocation, Latitude, Longitude, etc. We have taken only columns which are of importance for prediction of the result of elections.

Here df_trump_pre represents modified dataset of Donald Trump dataset and df_biden_pre represents modified dataset for Joe Biden dataset.
"""

df_trump_pre = df_trump.drop(['tweet_id','created_at','likes','retweet_count','user_location','lat','long','continent','collected_at','user_screen_name','user_description','city','created_at'],axis = 1)

df_biden_pre = df_biden.drop(['tweet_id','user_location','lat','likes','retweet_count','long','continent','collected_at','user_screen_name','user_description','city','created_at'],axis = 1)

"""Columns that are going to be used in the process correspond to Tweet, State of the person, user followers, platform used to publish tweet, etc."""

df_trump_pre.head()

df_biden_pre.head()

"""Now to analyse the combined dataset we have appended df_trump_pre with df_biden_pre"""

df_combine_pre = df_trump_pre.append(df_biden_pre)

"""# WordCloud from first 100000 words before preprocessing

We have taken 100000 words because it is of huge length
"""

# Here we drop rows that contains nan in any one of the ['tweet', 'source','user_id','user_name','user_join_date'] , just to avoid to nan's
df_combine_pre = df_combine_pre.dropna(subset=['tweet', 'source','user_id','user_name','user_join_date'])

paragraph = combined_paragraph = df_combine_pre['tweet'].str.cat(sep=' ')
paragraph = paragraph[:100000]
from wordcloud import WordCloud
wc = WordCloud(width=800,height=800,background_color='white',max_words=100, collocations = False).\
generate_from_text(paragraph)
plt.figure(figsize = [8,8])
plt.imshow(wc)
plt.show()

"""Now we will plot the number of users joining twitter vs how many Number of days before 3 Nov 2020 (which is the date of election) the user joined twitter. So that we can filter out the fake users who just want to support any of the party.
For that we prepare a column 'delta_days'
"""

date = pd.to_datetime(df_combine_pre['user_join_date'])
reference = pd.to_datetime('2020-11-23 00:00:00')
df_combine_pre['delta_days'] = reference-date
df_combine_pre['delta_days'] = df_combine_pre['delta_days'].dt.days

df_combine_pre.head()

import pandas as pd
import matplotlib.pyplot as plt

# Group by 'joining_date' and count the number of unique users
join_counts = df_combine_pre.groupby('delta_days')['user_name'].nunique()

# Plot the data
plt.figure(figsize=(10, 6))
plt.xlim(0, 5200)
plt.fill_between(join_counts.index, join_counts.values, color='skyblue', alpha=1)
plt.title('Number of Users Joining Over Time')
plt.xlabel('Number of days from 3 Nov 2020')
plt.ylabel('Number of Users')
plt.xticks(rotation=45)
plt.show()

"""
Dataset contains multiple names for the same country in the country column. So to avoid this we have replace all the multiple names for the country of interest to one name.
Also NaN values are replace by NA for further filtering"""

df_combine_pre['country'] = df_combine_pre['country'].replace(np.nan, 'NA')
df_combine_pre['country'] = df_combine_pre['country'].replace('United States', 'United States of America')
df_combine_pre['country'] = df_combine_pre['country'].replace('The Netherlands', 'Netherlands')

df_combine_pre['state'] = df_combine_pre['state'].replace(np.nan, 'NA')

df_combine_pre['state_code'] = df_combine_pre['state_code'].replace(np.nan, 'NA')

df_combine_pre.head()

"""
Here we plot the pie chart representing the proportion of tweets from each country."""

# Taking country column from the combine dataset
countryPie = df_combine_pre['country'].value_counts()

countryPie.to_frame().T

from matplotlib import pyplot as plt
import numpy as np
#Picking up countries with largest share of tweets for the pie-chart
keys = ['United States of America','United Kingdom','India','Germany','France','Canada','Italy','Australia','Netherlands','Mexico','Turkey','Brazil','Pakistan','Spain','Ireland','others']
values = []
AllPrimary = 0

#Storing number of tweets from each country
for i in range(len(keys)-1):
    temp = countryPie[keys[i]]
    values.append(temp)
    AllPrimary = AllPrimary + temp
values.append(len(df_combine_pre)-AllPrimary-countryPie['NA'])

#Country Pie Chart
fig = plt.figure(figsize =(8,8))
plt.rcParams['font.size'] = 10
plt.pie(values, labels =keys)

#Now, proceeding to pie-chart for platforms used for publishing tweets
sourcePie = df_combine_pre['source'].value_counts()

"""
Here a Pie-Chart is made for platforms used for publishing tweets"""

sourcePie.to_frame().T

"""Here we plot pie chart representing proportion of users from the source ['Twitter Web App','Twitter for iPhone','Twitter for Android','Twitter for iPad','Twitter for Mac','others']"""

from matplotlib import pyplot as plt
import numpy as np
keysSource = ['Twitter Web App','Twitter for iPhone','Twitter for Android','Twitter for iPad','Twitter for Mac','others']
valuesSource = []
AllPrimarySource = 0

#Storing number of tweets from each platform
for i in range(len(keysSource)-1):
    tempSource = sourcePie[keysSource[i]]
    valuesSource.append(tempSource)
    AllPrimarySource = AllPrimarySource + tempSource
valuesSource.append(len(df_combine_pre)-AllPrimarySource)

valuesSource

#Platform/Source Pie Chart
figSource = plt.figure(figsize =(8,8))
plt.rcParams['font.size'] = 10
plt.pie(valuesSource, labels =keysSource)

"""Election date was 2020-11-03

if account gets created after 2020-10-03 is more likely to be a fake one for campaigning. Hence we will not consider that in our model.
"""

# df_used is data frame obtained by removing recent tweeter accounts.
df_used = df_combine_pre[(df_combine_pre['user_join_date'] <= '2020-10-03')]
df_used.drop('user_join_date',axis = 1,inplace = True)

"""# After removing users from 3 Oct 2020 to 3 Nov 2020

After removing users from 3oct to 3 nov 2020 we ensures that we minimise the effect of spamming.
"""

df_x = df_combine_pre[(df_combine_pre['user_join_date'] <= '2020-10-03')]
date = pd.to_datetime(df_x['user_join_date'])
reference = pd.to_datetime('2020-11-23 00:00:00')
df_x['delta_days'] = reference-date
df_x['delta_days'] = df_x['delta_days'].dt.days
import pandas as pd
import matplotlib.pyplot as plt

# d = df_trump.append(df_biden)
# Convert 'joining_date' column to datetime type
# d['user_join_date'] = pd.to_datetime(d['user_join_date'])

# Group by 'joining_date' and count the number of unique users
join_counts = df_x.groupby('delta_days')['user_name'].nunique()

# Plot the data
plt.figure(figsize=(10, 6))
plt.fill_between(join_counts.index, join_counts.values, color='skyblue', alpha=1)
plt.title('Number of Users Joining Over Time')
plt.xlabel('Number of days from 3 Nov 2020')
plt.ylabel('Number of Users')
plt.xticks(rotation=45)
plt.show()

df_used = df_used[df_used['country'] == 'United States of America']
df_used.drop('country',axis = 1,inplace = True)

# array contains those sources which we will be considering in our analysis
array = ['Twitter for iPad','Twitter for iPhone','Twitter Web App','Twitter for Mac','Twitter for Android']

df_used = df_used[(df_used['source'] == array[0]) | (df_used['source'] == array[1]) | (df_used['source'] == array[2]) | (df_used['source'] == array[3]) | (df_used['source'] == array[4])]
df_used.drop('source',axis = 1,inplace = True)

"""Those users whose followers count is less than 10 are removed from the estimation because they might not be of that much importance (It is an assumption). Threshold may be changed according to our convinience."""

df_used = df_used[df_used['user_followers_count'] >= 10]
df_used.drop('user_followers_count',axis = 1,inplace = True)

df_used.drop('user_id',axis = 1,inplace = True)

df_used=df_used.reset_index()
df_used.drop('index',axis = 1,inplace = True)

df_used.head()

"""Now we need to preprocess the tweets to make them ready for our analysis

Here we import libraries that are important for text analysis. And along with that we are using some english stop words and punctuations to avoid them in the tweets.
"""

import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
stops=set(stopwords.words('english'))
punctuations=list(string.punctuation)
stops.update(punctuations)

"""Cleaning function will clean all the tweets from the combined dataset one by one by removing stop words and unnecessary things from the sentences."""

def cleaning(sentence):
    tokenize = word_tokenize(sentence)
    word = []
    for i in tokenize:
        if i.lower() not in stops:
            word.append(i) #Removed the stop words utilizing the NLTK library.
    s = " ".join(word)
    s = re.sub('n\'t',"not",s)
    return s

# This function will be helpful in making sentence more clear by removing unnecessary things like underscore(_), hashtags(#), @.
# It also converts “snake_case”, “camelCase”, and “PascalCase” to “normal case”
import re
def case_split(str1):
    snake_case_string = ""
    str1 = re.sub('#',"",str1)
    str1 = re.sub('@',"",str1)
    for i, c in enumerate(str1):
        if i == 0:
            snake_case_string += c.lower()
        elif c.isupper():
            snake_case_string += "_" + c.lower()
        else:
            snake_case_string += c
    str1 = re.sub('_'," ",snake_case_string)

    return str1

""" Here we are using contractions library to expand contractions,
 like if the text contains cannot it will be replaced by can not
"""

import contractions
#Expanded short forms such as “can’t” , “I’m” to their corresponding full forms “can not” and “I am”
def expand_contractions(text):
    text = case_split(text)
    expanded_text = text
    if (text == 'n\'t'):
        return "not"

    # Handle specific cases
    expanded_text = expanded_text.replace("cannot", "can not")
    expanded_text = expanded_text.replace("gonna", "going to")
    expanded_text = expanded_text.replace("wanna", "want to")
    expanded_text = expanded_text.replace("gotta", "got to")
    expanded_text = expanded_text.replace("ain't", "am not")
    expanded_text = expanded_text.replace("aren't", "are not")
    expanded_text = expanded_text.replace("can't", "can not")
    expanded_text = expanded_text.replace("couldn't", "could not")
    expanded_text = expanded_text.replace("didn't", "did not")
    expanded_text = expanded_text.replace("doesn't", "does not")
    expanded_text = expanded_text.replace("don't", "do not")
    expanded_text = expanded_text.replace("hadn't", "had not")
    expanded_text = expanded_text.replace("hasn't", "has not")
    expanded_text = expanded_text.replace("haven't", "have not")
    expanded_text = expanded_text.replace("he'd", "he would")
    expanded_text = expanded_text.replace("he'll", "he will")
    expanded_text = expanded_text.replace("he's", "he is")
    expanded_text = expanded_text.replace("i'd", "i would")
    expanded_text = expanded_text.replace("i'll", "i will")
    expanded_text = expanded_text.replace("i'm", "i am")
    expanded_text = expanded_text.replace("i've", "i have")
    expanded_text = expanded_text.replace("isn't", "is not")
    expanded_text = expanded_text.replace("it's", "it is")
    expanded_text = expanded_text.replace("let's", "let us")
    expanded_text = expanded_text.replace("mustn't", "must not")
    expanded_text = expanded_text.replace("shan't", "shall not")
    expanded_text = expanded_text.replace("she'd", "she would")
    expanded_text = expanded_text.replace("she'll", "she will")
    expanded_text = expanded_text.replace("she's", "she is")
    expanded_text = expanded_text.replace("shouldn't", "should not")
    expanded_text = expanded_text.replace("that's", "that is")
    expanded_text = expanded_text.replace("there's", "there is")
    expanded_text = expanded_text.replace("they'd", "they would")
    expanded_text = expanded_text.replace("they'll", "they will")
    expanded_text = expanded_text.replace("they're", "they are")
    expanded_text = expanded_text.replace("they've", "they have")
    expanded_text = expanded_text.replace("we'd", "we would")
    expanded_text = expanded_text.replace("we'll", "we will")
    expanded_text = expanded_text.replace("we're", "we are")
    expanded_text = expanded_text.replace("we've", "we have")
    expanded_text = expanded_text.replace("weren't", "were not")
    expanded_text = expanded_text.replace("what'll", "what will")
    expanded_text = expanded_text.replace("what're", "what are")
    expanded_text = expanded_text.replace("what's", "what is")
    expanded_text = expanded_text.replace("what've", "what have")
    expanded_text = expanded_text.replace("where's", "where is")
    expanded_text = expanded_text.replace("who'll", "who will")
    expanded_text = expanded_text.replace("who's", "who is")
    expanded_text = expanded_text.replace("won't", "will not")
    expanded_text = expanded_text.replace("wouldn't", "would not")
    expanded_text = expanded_text.replace("you'd", "you would")
    expanded_text = expanded_text.replace("you'll", "you will")
    expanded_text = expanded_text.replace("you're", "you are")
    expanded_text = expanded_text.replace("you've", "you have")
    # We can add more contractions according our requirements

    return expanded_text

from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize

#Tokenized the text and replaced the words preceded by “not” with their antonyms to make sure that “not” is not removed during text cleaning.
def replace_antonyms(sentence):
    sentence = expand_contractions(sentence)
    words = word_tokenize(sentence)
    new_words = []

    i = 0
    while i < len(words):
        if words[i].lower() == "not" and i + 1 < len(words):
            antonyms = []
            for syn in wordnet.synsets(words[i + 1]):
                for lemma in syn.lemmas():
                    if lemma.antonyms():
                        antonyms.append(lemma.antonyms()[0].name())

            if antonyms:
                new_words.append(antonyms[0])
            else:
                new_words.extend([words[i], words[i + 1]])
            i += 2
        else:
            if (len(words[i])>1):
                new_words.append(words[i])
            i += 1

    new_sentence = ' '.join(new_words)
    return new_sentence

#Main function calling other function to clean tweet data
def clean_tweets(text):
    text = replace_antonyms(text)
    text = cleaning(text)
    return text

df_used['tweet'] = df_used['tweet'].apply(lambda x:clean_tweets(x))

"""# WordCloud after cleaning tweets"""

paragraph = df_used['tweet'].str.cat(sep=' ')
paragraph = paragraph[:100000]
wc = WordCloud(width=800,height=800,background_color='white',max_words=100, collocations = False).\
generate_from_text(paragraph)
plt.figure(figsize = [8,8])
plt.imshow(wc)
plt.show()

"""NLP(Natural language toolkit)"""

import nltk #importing nltk library
from nltk.sentiment.vader import SentimentIntensityAnalyzer # for vader analysis we have import this library
#nltk.download('vader_lexicon')
sid = SentimentIntensityAnalyzer()

df_used['sentiment'] = df_used['tweet'].apply(lambda x: sid.polarity_scores(x)["compound"])
#storing compound value in sentiment column
def sentimentalPrediction(sentiment):
    if sentiment >= 0.05:
        return "Positive"
    elif sentiment <= -0.05:
        return "Negative"
    else:
        return "Neutral"
df_used['sentiment_val'] = df_used['sentiment'].apply(lambda x: sentimentalPrediction(x))

df_used

df_used = df_used.sort_values(by = 'user_name') # Sorting the df_values by user_names
df_used=df_used.reset_index() #reseting index
df_used.drop('index',axis = 1,inplace = True)
df_used

userList = list(df_used['user_name'])
userUniqueList = list(df_used['user_name'].unique()) #list for unique users
df_final = pd.DataFrame({ 'UserName':["u" for user in userUniqueList],'Compound_val':[0.0 for user in userUniqueList],'Vote':["v" for user in userUniqueList]})

df_final #created for vote counting

#userList and df_used is sorted (putting user names in alphabetical order)
ptr = userList[0] #ptr is pointer for iterating through usernames
compound1 = 0.0 #variable for computing average compound value of sentiment
ii = 0 #second pointer for index of df_used and userList
pt2 = 0 #variable for index of df_final
for i in range(len(userList)):
    ptr = userList[i]
    sub = df_used['Tweet_subject'][i]
    if (sub == 'Trump'):
        compound1 = compound1 + df_used['sentiment'][i] #taking positive sign for trump
    else: #biden
        compound1 = compound1 - df_used['sentiment'][i] #taking positive sign for biden

    if (ptr == userList[ii]): #we are analysing data for same user
        continue
    #updating data for user we finished generating data for
    temp = i-ii #number of tweets done by the user
    df_final['Compound_val'][pt2] = compound1/temp #average sentiment value
    df_final['UserName'][pt2] = userList[ii] #Storing user name
    y = compound1/temp
    if (y > 0.05):
        df_final['Vote'][pt2] = "Trump"
    elif (y < -0.05):
        df_final['Vote'][pt2] = "Biden"
    else: # for values of average sentiment [-0.05,0.05] nothing can be predicted
        df_final['Vote'][pt2] = "None"
    #moved on to next user
    compound1 = 0.0
    ii = i #second pointer mapped to first pointer
    pt2 = pt2+1 #incremented variable for index of df_final

df_final.head()

df_preprocessed = df_final.copy()

df_preprocessed.head()

"""# Estimated percentage of votes to Trump and Biden and neutral

From the df_preprocessed data frame, we will calculate total voters who are likely to vote Trump and Biden .
"""

trump_total_votes = (df_preprocessed['Vote'] == 'Trump').sum()
biden_total_votes = (df_preprocessed['Vote'] == 'Biden').sum()
neutral_votes = (df_preprocessed['Vote'] == 'None').sum() # these number of voters are tough to predict whether they will vote Trump or Biden
total_votes = (trump_total_votes+biden_total_votes+neutral_votes)

print('Donald Trump estimated vote% is',trump_total_votes*100/total_votes)
print('Joe Biden estimated vote% is',biden_total_votes*100/total_votes)
print('Estimated Percentage of neutral public is',neutral_votes*100/total_votes)

"""# Pie Chart for the estimated distribution of voters"""

import matplotlib.pyplot as plt

# Sample data
labels = ['Trump', 'Biden', 'Neutral']
sizes = [trump_total_votes, biden_total_votes, neutral_votes]  # Percentages should add up to 100

# Create a pie chart
plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=['skyblue', 'lightcoral', 'lightgreen', 'lightsalmon'])
plt.title('Distribution of Categories')

# Show the plot
plt.show()

"""# State wise estimation of votes to Trump and Biden #"""

#state_codes array
stateCodes = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']
# dictionary having mapping of state_codes with states
stateMapping = {'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California', 'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',
                  'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland', 'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NY': 'New York', 'NM': 'New Mexico', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina', 'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT':  'Utah', 'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington', 'WV':  'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'}

df_state_wise_voters = pd.DataFrame({'TrumpSupport':[0 for s in stateCodes],'BidenSupport':[0 for s in stateCodes],'Neutral':[0 for s in stateCodes]},index = [stateMapping[s] for s in stateCodes])
d = dict(zip(list(df_trump['user_name']),list(df_trump['state_code'])))
d.update(dict(zip(list(df_biden['user_name']),list(df_biden['state_code']))))
def apply_transformation(x):
    if d.get(x) == None:
        return np.nan
    return d[x]
df_preprocessed['state'] = df_preprocessed['UserName'].apply(lambda x: apply_transformation(x))
df_preprocessed.dropna(inplace = True)
df_preprocessed.reset_index(drop = True,inplace = True)
for i in stateCodes:
    df_state_wise_voters.loc[stateMapping[i],'TrumpSupport'] = ((df_preprocessed['state'] == i)&(df_preprocessed['Vote'] == 'Trump')).sum()
    df_state_wise_voters.loc[stateMapping[i],'BidenSupport'] = ((df_preprocessed['state'] == i)&(df_preprocessed['Vote'] == 'Biden')).sum()
    df_state_wise_voters.loc[stateMapping[i],'Neutral'] = ((df_preprocessed['state'] == i)&(df_preprocessed['Vote'] == 'None')).sum()

df_state_wise_voters

import pandas as pd
import matplotlib.pyplot as plt


# Plotting the bar plot
df_state_wise_voters.plot(kind='bar', figsize=(10, 6))
plt.title('Votes Count for Candidates Across States')
plt.xlabel('States')
plt.ylabel('Votes Count')
plt.legend(title='Candidates')
plt.show()